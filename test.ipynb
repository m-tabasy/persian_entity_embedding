{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\entity_embedding\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading 10000 word vectors from pretrained word2vec file at D:/_IUST/entity_embedding/data/cc.fa.300.vec... done!\n",
      "-- forcing generate unigram!\n",
      "-- generating unigram based on D:/_IUST/entity_embedding/data/fawiki-first-50000-docs.txt corpus... done!\n",
      "-- generating powered unigram on 8857 words... done!\n",
      "-- detecting stop words based on D:/_IUST/entity_embedding/data/fawiki-first-50000-docs.txt corpus... done!\n",
      "Wall time: 28min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from basic.words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- forcing generate entity id maps!\n",
      "-- looking for entity articles in D:/_IUST/entity_embedding/data/fawiki-first-50000-docs.txt corpus... done!\n",
      "   10000 entities added!\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from basic.entities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- starting first pass. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_times' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_times' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from data_gen import gen_wiki_canonical as can\n",
    "from importlib import reload\n",
    "\n",
    "reload(can)\n",
    "\n",
    "# can.gen_canonical_words(can.fa_wiki_path, can.canonical_words_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- generating hyperlink context data based on D:/_IUST/entity_embedding/data/fawiki-random-1000-docs.txt corpus... done!\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from data_gen import gen_wiki_hyp_ctx as hyp\n",
    "from importlib import reload\n",
    "\n",
    "reload(hyp)\n",
    "\n",
    "hyp.gen_wiki_hyp_ctx(hyp.fa_wiki_path, hyp.hyp_ctx_words_path, context_size=hyp.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- starting 1-th epoch!\n",
      "-- starting first pass. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 2-th pass on D:/_IUST/persian_entity_embedding/data/wiki-canonical-word-ids.txt. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 1-th pass on D:/_IUST/persian_entity_embedding/data/hyp-ctx-word-ids.txt. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- loss: 0.03536740317940712\n",
      "\n",
      "-- starting 2-th epoch!\n",
      "-- starting first pass. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 2-th pass on D:/_IUST/persian_entity_embedding/data/wiki-canonical-word-ids.txt. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 1-th pass on D:/_IUST/persian_entity_embedding/data/hyp-ctx-word-ids.txt. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- loss: 0.03175646439194679\n",
      "\n",
      "-- saving model!\n",
      "-- starting 3-th epoch!\n",
      "-- starting first pass. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 2-th pass on D:/_IUST/persian_entity_embedding/data/wiki-canonical-word-ids.txt. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 1-th pass on D:/_IUST/persian_entity_embedding/data/hyp-ctx-word-ids.txt. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- loss: 0.030237725004553795\n",
      "\n",
      "-- starting 4-th epoch!\n",
      "-- starting first pass. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 2-th pass on D:/_IUST/persian_entity_embedding/data/wiki-canonical-word-ids.txt. . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- starting 1-th pass on D:/_IUST/persian_entity_embedding/data/hyp-ctx-word-ids.txt. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . done!\n",
      "-- loss: 0.029296929016709328\n",
      "\n",
      "-- saving model!\n",
      "Wall time: 51min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from entity2vec import minibatch\n",
    "import main \n",
    "from entity2vec import train\n",
    "\n",
    "reload(minibatch)\n",
    "reload(main)\n",
    "reload(train)\n",
    "\n",
    "train.train_entity_vectors(main.network, main.loss_func, main.optimizer, epochs=4)\n",
    "\n",
    "# batch_count = 0\n",
    "# for inputs, targets in minibatch.gen_minibatch():\n",
    "#     batch_count += 1\n",
    "#     print(minibatch._times)\n",
    "#     if batch_count == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.599225759506226"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_time = 15.8 \n",
    "times_sum = 35.77\n",
    "# sum(minibatch._times)\n",
    "s = 0\n",
    "for i in minibatch._times:\n",
    "    s += i\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic.words import *\n",
    "from basic.entities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "موجودیت «قرآن» : کلمات قرآن, روایات, کریم, احادیث, حدیث, آیات, سوره, قرآنی, سنت, یوسف\n",
      "موجودیت «محمدرضا شجریان» : کلمات شجریان, آهنگساز, آهنگسازی, استاد, موسیقی, آواز, هنرمند, استادان, نوازنده, هنرمندان\n",
      "موجودیت «تبریز» : کلمات تبریز, آذربایجان, هجری, Banbao, روستای, Shahre, دورهٔ, آذری, زادگاه, شهر\n",
      "موجودیت «ارسطو» : کلمات فلسفه, فلسفی, فیلسوف, یونانی, نظریه, فرضیه, عقاید, منطق, اندیشه, مفهوم\n",
      "موجودیت «آلمان» : کلمات آلمان, آلمانی, فرانسه, معمار, امپراتوری, مجارستان, اروپایی, اتریش, لهستان, آکادمی\n",
      "موجودیت «اسلام» : کلمات اسلام, مسلمانان, مسیحیت, مذهب, سنت, مذاهب, دین, ادیان, مسلمان, تمدن\n",
      "موجودیت «محمدرضا فروتن» : کلمات بازیگر, حاتمی, جواد, کارگردان, حمیدرضا, بازیگران, بهروز, مهسا, لیلا, کارگردانی\n",
      "موجودیت «شهاب حسینی» : کلمات فیلم, بازیگر, سینمایی, کارگردان, فیلمی, داستان, هنرمند, فیلمهای, جشنواره, فرهادی\n",
      "موجودیت «محمدرضا شفیعی کدکنی» : کلمات غلامحسین, محمدحسین, محمدرضا, محمود, علامه, حسین, غلامرضا, محمدعلی, اکبر, ابوالقاسم\n"
     ]
    }
   ],
   "source": [
    "ent_names = ['قرآن', 'محمدرضا شجریان', 'تبریز', 'ارسطو', 'آلمان', 'اسلام', 'محمدرضا فروتن', 'شهاب حسینی', 'محمدرضا شفیعی کدکنی']\n",
    "\n",
    "ent_ids = list(map(lambda e: wikiid2id[name2wikiid[e]], ent_names))\n",
    "\n",
    "ent_ids = torch.Tensor(ent_ids).long()\n",
    "\n",
    "embeddings = main.network.embedding(ent_ids).detach().numpy()\n",
    "\n",
    "for i, e in enumerate(embeddings):\n",
    "\n",
    "    similars = word2vec.similar_by_vector(e)\n",
    "    words = []\n",
    "    for s in similars:\n",
    "        words.append(s[0])\n",
    "        \n",
    "    print(f'موجودیت «{ent_names[i]}» : کلمات {\", \".join(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('فروتن', 0.9123691320419312),\n",
       " ('محمدرضا', 0.7368292808532715),\n",
       " ('مسعود', 0.5714483261108398),\n",
       " ('رضا', 0.5569769144058228),\n",
       " ('جواد', 0.5502303838729858),\n",
       " ('مجید', 0.5325300693511963),\n",
       " ('محمد', 0.5321899652481079),\n",
       " ('حمیدرضا', 0.5263552069664001),\n",
       " ('شریفی', 0.5103566646575928),\n",
       " ('حمید', 0.5052734613418579)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = word2vec.get_vector('محمدرضا')\n",
    "second = word2vec.get_vector('فروتن')\n",
    "\n",
    "word2vec.similar_by_vector((first + second)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('شهاب', 0.9249618649482727),\n",
       " ('حسینی', 0.8233593702316284),\n",
       " ('مرادی', 0.554496705532074),\n",
       " ('کاظمی', 0.5367589592933655),\n",
       " ('مظفری', 0.536064088344574),\n",
       " ('حسین', 0.5333384871482849),\n",
       " ('اکبری', 0.5286636352539062),\n",
       " ('سید', 0.51639324426651),\n",
       " ('مهدی', 0.5141152143478394),\n",
       " ('کیوان', 0.5048292875289917)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = word2vec.get_vector('شهاب')\n",
    "second = word2vec.get_vector('حسینی')\n",
    "\n",
    "word2vec.similar_by_vector((first + second)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_names = ['قرآن', 'محمدرضا شجریان', 'تبریز', 'ارسطو', 'آلمان', 'اسلام']\n",
    "\n",
    "ent_ids = list(map(lambda e: wikiid2id[name2wikiid[e]], ent_names))\n",
    "\n",
    "ent_ids = torch.Tensor(ent_ids).long()\n",
    "\n",
    "embeddings = main.network.embedding(ent_ids).detach().numpy()\n",
    "\n",
    "for i, e in enumerate(embeddings):\n",
    "\n",
    "    similars = word2vec.similar_by_vector(e)\n",
    "    words = []\n",
    "    for s in similars:\n",
    "        words.append(s[0])\n",
    "        \n",
    "    print(f'-- {ent_names[i]} is similar to : {\", \".join(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\entity_embedding\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading 100000 word vectors from pretrained word2vec file at D:/_IUST/entity_embedding/data/cc.fa.300.vec... done!\n",
      "-- generating unigram based on D:/_IUST/entity_embedding/data/fawiki-first-50000-docs.txt corpus... done!\n",
      "-- generating powered unigram on 61095 words... done!\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('entity2vec/model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5, 6).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can only calculate the mean of floating types. Got Long instead.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-815dc759800f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Can only calculate the mean of floating types. Got Long instead."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "t.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "reload(torch)\n",
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'بر اساس چهارچوب پروژه‌های چندزبانهٔ بنیادِ <a href=\"%D9%88%DB%8C%DA%A9%DB%8C%E2%80%8C%D9%85%D8%AF%DB%8C%D8%A7\">ویکی‌مدیا</a>، در زبان فارسی نیز از یک نرم‌افزار ویکی استفاده شده‌است. برخی از پروژه‌های فارسیِ این بنیاد از این قرارند:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اساس\tپروژه‌های\tزبان\tفارسی\tنرم‌افزار\tویکی\tاستفاده\tشده‌است\tپروژه‌های\tبنیاد\n"
     ]
    }
   ],
   "source": [
    "from hazm import word_tokenize\n",
    "from gensim.corpora import wikicorpus\n",
    "\n",
    "ss = wikicorpus.filter_wiki(s)\n",
    "sss = word_tokenize(ss)\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "valid_words = list(filter(lambda w: w in word2id.keys(), sss))\n",
    "pure_words = list(filter(lambda w: w not in stop_words, valid_words))\n",
    "\n",
    "print('\\t'.join(pure_words))\n",
    "\n",
    "with open('text.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\t'.join(pure_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "bbb\n",
      "cccc\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-17d1e8086b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "a = 'aa'\n",
    "b = 'bbb'\n",
    "c = 'cccc'\n",
    "\n",
    "with open('test.pcl', 'wb') as f:\n",
    "    pickle.dump(a, f)\n",
    "    pickle.dump(b, f)\n",
    "    pickle.dump(c, f)\n",
    "    \n",
    "with open('test.pcl', 'rb') as f:\n",
    "    print(pickle.load(f))\n",
    "    print(pickle.load(f))\n",
    "    print(pickle.load(f))\n",
    "    print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(1500000):\n",
    "    if str(i) in d1:\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(1500000):\n",
    "    if str(i) in d2:\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(1500000):\n",
    "    if str(i) in d1.keys():\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(1500000):\n",
    "    if str(i) in d2.keys():\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(1500000):\n",
    "    if i < len(l):\n",
    "        x += 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17],\n",
       "        [18, 19, 20]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(7, 3).long()\n",
    "tgs = torch.empty(7).long()\n",
    "ids = torch.empty(100).long()\n",
    "idxs = torch.empty(7).long()\n",
    "\n",
    "for i in range(7):\n",
    "    tgs[i] = i % 3\n",
    "    idxs[i] = i+j\n",
    "    for j in range(3):\n",
    "        x[i][j] = 3*i+j\n",
    "\n",
    "        \n",
    "for i in range(100):\n",
    "    ids[i] = 100 + i\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102, 103, 104, 105, 106, 107, 108])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  4,  8,  9, 13, 17, 18])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_idxs = torch.arange(0, 7)* 3 + tgs\n",
    "flat_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[102,   1,   2],\n",
       "        [  3, 103,   5],\n",
       "        [103,   7, 104],\n",
       "        [105,  10,  11],\n",
       "        [104, 106,  14],\n",
       "        [105,  16, 107],\n",
       "        [108,  19,  20]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.put_(flat_idxs, ids[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 356 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(500):\n",
    "    np.random.choice(np.arange(10000), size=20, p=np.ones(10000) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(500):\n",
    "    np.random.choice(np.arange(500000), size=20, p=np.ones(500000) / 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool(processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- starting first pass. "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4ee93ddd7c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mbatch_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\_IUST\\persian_entity_embedding\\entity2vec\\minibatch.py\u001b[0m in \u001b[0;36mgen_minibatch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                                         \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                                 \u001b[0mdata_available\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\entity_embedding\\lib\\threading.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    849\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m_limbo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\entity_embedding\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\entity_embedding\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "batch_count = 0\n",
    "for inputs, targets in minibatch.gen_minibatch():\n",
    "\tbatch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
