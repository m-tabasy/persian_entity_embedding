[path]
# data
wiki = D:/_IUST/entity_embedding/data/fawiki-first-50000-docs.txt
word2vec = D:/_IUST/entity_embedding/data/cc.fa.300.vec
canonical = D:/_IUST/persian_entity_embedding/data/wiki-canonical-word-ids.txt
hyp_ctx = D:/_IUST/persian_entity_embedding/data/hyp-ctx-word-ids.txt

# pickled objects
word = D:/_IUST/persian_entity_embedding/basic/words.pickle
entity = D:/_IUST/persian_entity_embedding/basic/entities.pickle
model = D:/_IUST/persian_entity_embedding/entity2vec/model.pickle

[limit]
word = 10000
entity = 10000

[param]
# data gen params
regenerate = true
hyp_ctx_len = 10
unig_power = 0.6

# model params
vector_size = 300
words_per_ent = 20
neg_words = 5

# training params
lr = 0.3
margin = 0.1
epochs = 4
thread = 4
batch_size = 500
passes_wiki_words = 2
